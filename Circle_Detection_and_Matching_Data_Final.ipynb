{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Create Files"
      ],
      "metadata": {
        "id": "SrcY64Gr0z3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the base directory and the folder names\n",
        "base_dir = '/content'\n",
        "folders = ['images1', 'images2', 'images3', 'selectedpi']\n",
        "\n",
        "# Create the directories if they don't already exist\n",
        "for folder in folders:\n",
        "    folder_path = os.path.join(base_dir, folder)\n",
        "    os.makedirs(folder_path, exist_ok=True)\n",
        "    print(f\"Directory created: {folder_path}\")\n"
      ],
      "metadata": {
        "id": "T_gKWIrb0-if"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Delete Files"
      ],
      "metadata": {
        "id": "-Pgl7-W-1GBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Specify the folder path to delete\n",
        "folder_path = '/content/selectedpi'  # Replace with your folder path\n",
        "\n",
        "# Delete the folder\n",
        "try:\n",
        "    shutil.rmtree(folder_path)\n",
        "    print(f\"Folder '{folder_path}' has been deleted.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Folder '{folder_path}' does not exist.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "94_4VxEh1F3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "File Number"
      ],
      "metadata": {
        "id": "jWPge1p408Wn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def count_files_in_folder(folder_path):\n",
        "    file_count = 0\n",
        "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
        "        file_count += len(filenames)  # Count the number of files in the current directory\n",
        "    return file_count\n",
        "\n",
        "# Specify the path to the folder\n",
        "folder_path1 = '/content/selectedpi'  # Replace with your folder path\n",
        "folder_path2 = '/content/images2'  # Replace with your folder path\n",
        "folder_path3 = '/content/images3'  # Replace with your folder path\n",
        "\n",
        "# Get the number of files in the folder\n",
        "num_files1 = count_files_in_folder(folder_path1)\n",
        "num_files2 = count_files_in_folder(folder_path2)\n",
        "num_files3 = count_files_in_folder(folder_path3)\n",
        "\n",
        "\n",
        "# Print the result\n",
        "print(f\"Number of files in the folder: {num_files1}\")\n",
        "print(f\"Number of files in the folder: {num_files2}\")\n",
        "print(f\"Number of files in the folder: {num_files3}\")\n"
      ],
      "metadata": {
        "id": "PCGbPoNS0zt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for Contour and Area Analysis"
      ],
      "metadata": {
        "id": "mqrIapTb0uOQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0-mIGIp0p2J"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial import distance\n",
        "import re\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "# Define constants and paths\n",
        "PIXELS_TO_MICROMETERS = 9.2667  # Conversion factor\n",
        "image_dirs = ['/content/images1', '/content/images2', '/content/images3']\n",
        "thresholded_dir = '/content/thresholded_images'\n",
        "contour_dir = '/content/contour_images'\n",
        "excel_dir = '/content/excel_files'\n",
        "selected_pi_dir = '/content/selectedpi'\n",
        "output_zip = '/content/processed_results.zip'\n",
        "selected_pi_excel = '/content/selected_pi_details.xlsx'\n",
        "sorted_excel_file = '/content/sorted_selected_pi_details.xlsx'\n",
        "diffusion_file = '/content/Diffusion_coefficients.txt'\n",
        "final_output_file = '/content/matched_particle_data_with_difference.csv'\n",
        "\n",
        "# Create necessary directories\n",
        "os.makedirs(thresholded_dir, exist_ok=True)\n",
        "os.makedirs(contour_dir, exist_ok=True)\n",
        "os.makedirs(excel_dir, exist_ok=True)\n",
        "\n",
        "# Step 1: Process images\n",
        "def process_and_analyze_image(image_path, thresholded_path, contour_path, excel_path):\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    adaptive_thresh = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
        "\n",
        "    # Save thresholded image with reduced quality\n",
        "    cv2.imwrite(thresholded_path, adaptive_thresh, [int(cv2.IMWRITE_JPEG_QUALITY), 30])  # Quality = 30\n",
        "\n",
        "    # Morphological cleaning\n",
        "    kernel = np.ones((5, 5), np.uint8)\n",
        "    cleaned_image = cv2.morphologyEx(adaptive_thresh, cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "    # Contour detection and analysis\n",
        "    contours, _ = cv2.findContours(cleaned_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    data = []\n",
        "    for contour in contours:\n",
        "        area_pixels = cv2.contourArea(contour)\n",
        "        if area_pixels > 10:\n",
        "            area_micrometers = area_pixels / (PIXELS_TO_MICROMETERS**2)\n",
        "            (x, y), radius_pixels = cv2.minEnclosingCircle(contour)\n",
        "            radius_micrometers = radius_pixels / PIXELS_TO_MICROMETERS\n",
        "            data.append({\n",
        "                \"Area (µm²)\": area_micrometers,\n",
        "                \"Center X (pixels)\": x,\n",
        "                \"Center Y (pixels)\": y,\n",
        "                \"Radius (µm)\": radius_micrometers\n",
        "            })\n",
        "\n",
        "    # Save data to Excel\n",
        "    pd.DataFrame(data).to_excel(excel_path, index=False)\n",
        "\n",
        "    # Save image with contours\n",
        "    result_image = cv2.cvtColor(cleaned_image, cv2.COLOR_GRAY2BGR)\n",
        "    for row in data:\n",
        "        center_x = int(row[\"Center X (pixels)\"])\n",
        "        center_y = int(row[\"Center Y (pixels)\"])\n",
        "        radius = int(row[\"Radius (µm)\"] * PIXELS_TO_MICROMETERS)\n",
        "        cv2.circle(result_image, (center_x, center_y), radius, (0, 255, 0), 2)\n",
        "    cv2.imwrite(contour_path, result_image, [int(cv2.IMWRITE_JPEG_QUALITY), 30])  # Quality = 30\n",
        "\n",
        "for image_dir in image_dirs:\n",
        "    for filename in os.listdir(image_dir):\n",
        "        if filename.endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff')):\n",
        "            input_path = os.path.join(image_dir, filename)\n",
        "            thresholded_path = os.path.join(thresholded_dir, f'thresholded_{filename}')\n",
        "            contour_path = os.path.join(contour_dir, f'contour_{filename}')\n",
        "            excel_path = os.path.join(excel_dir, f'{os.path.splitext(filename)[0]}_analysis.xlsx')\n",
        "            process_and_analyze_image(input_path, thresholded_path, contour_path, excel_path)\n",
        "\n",
        "# Step 2: Process Selected PI files\n",
        "data = []\n",
        "for filename in os.listdir(selected_pi_dir):\n",
        "    if filename.endswith('.txt') and filename.startswith('selected_PI_'):\n",
        "        match = re.search(r'\\d+', filename.split('_')[2])\n",
        "        if match:\n",
        "            particle_id = int(match.group())\n",
        "            pi_file_path = os.path.join(selected_pi_dir, filename)\n",
        "            pi_data = pd.read_csv(pi_file_path, delim_whitespace=True, header=None, nrows=1)\n",
        "            x, y, frame = pi_data.iloc[0, :3]\n",
        "            data.append([particle_id, x, y, frame])\n",
        "\n",
        "pd.DataFrame(data, columns=[\"Particle ID\", \"X\", \"Y\", \"Frame\"]).to_excel(selected_pi_excel, index=False)\n",
        "\n",
        "# Step 3: Sort Selected PI data\n",
        "selected_pi_data = pd.read_excel(selected_pi_excel)\n",
        "selected_pi_data.sort_values(by=\"Particle ID\").reset_index(drop=True).to_excel(sorted_excel_file, index=False)\n",
        "\n",
        "# Step 4: Add Diffusion Coefficients\n",
        "selected_pi_data = pd.read_excel(sorted_excel_file)\n",
        "diffusion_data = pd.read_csv(diffusion_file, delim_whitespace=True, header=None, names=[\"Particle ID\", \"Diffusion Coefficient\"])\n",
        "selected_pi_data[\"Diffusion Coefficient\"] = selected_pi_data[\"Particle ID\"].map(\n",
        "    diffusion_data.set_index(\"Particle ID\")[\"Diffusion Coefficient\"]\n",
        ")\n",
        "\n",
        "# Step 5: Match with Excel analysis\n",
        "results = []\n",
        "\n",
        "# Map frame numbers to Excel files from `excel_dir`\n",
        "frame_to_file = {}\n",
        "for f in os.listdir(excel_dir):\n",
        "    match = re.search(r't(\\d+)_analysis\\.xlsx', f)\n",
        "    if match:\n",
        "        frame_number = int(match.group(1))\n",
        "        frame_to_file[frame_number] = os.path.join(excel_dir, f)\n",
        "\n",
        "# Process each row in `selected_pi_data` and match with the closest particle\n",
        "for _, row in selected_pi_data.iterrows():\n",
        "    particle_id, x, y, frame = row[\"Particle ID\"], row[\"X\"], row[\"Y\"], row[\"Frame\"]\n",
        "    excel_file = frame_to_file.get(frame)\n",
        "    if excel_file and os.path.exists(excel_file):\n",
        "        data = pd.read_excel(excel_file)\n",
        "        data['Distance'] = data.apply(\n",
        "            lambda r: distance.euclidean((r[\"Center X (pixels)\"], r[\"Center Y (pixels)\"]), (x, y)), axis=1\n",
        "        )\n",
        "        closest = data.loc[data['Distance'].idxmin()]\n",
        "        results.append([\n",
        "            particle_id, x, y, frame, closest[\"Center X (pixels)\"], closest[\"Center Y (pixels)\"],\n",
        "            closest[\"Area (µm²)\"], closest[\"Radius (µm)\"], row[\"Diffusion Coefficient\"], closest[\"Distance\"]\n",
        "        ])\n",
        "\n",
        "columns = [\n",
        "    \"Particle ID\", \"Original X\", \"Original Y\", \"Frame\",\n",
        "    \"Matched X\", \"Matched Y\", \"Area (µm²)\", \"Radius (µm)\",\n",
        "    \"Diffusion Coefficient\", \"Computed Difference\"\n",
        "]\n",
        "\n",
        "# Save the matched data to CSV\n",
        "pd.DataFrame(results, columns=columns).to_csv(final_output_file, index=False)\n",
        "\n",
        "# Ensure the matched file is visible in the sidebar\n",
        "print(f\"Matched file saved to: {final_output_file}\")\n",
        "\n",
        "# Download the matched file automatically\n",
        "files.download(final_output_file)\n",
        "\n",
        "# Optional: Print files in the directory to confirm visibility in the sidebar\n",
        "print(\"\\nFiles in the /content directory:\")\n",
        "for f in os.listdir('/content'):\n",
        "    print(f)\n",
        "\n",
        "# Step 6: Create and Split ZIP\n",
        "with zipfile.ZipFile(output_zip, 'w') as zipf:\n",
        "    for folder in [thresholded_dir, contour_dir, excel_dir]:\n",
        "        for filename in os.listdir(folder):\n",
        "            filepath = os.path.join(folder, filename)\n",
        "            zipf.write(filepath, arcname=filepath.replace('/content/', ''))\n",
        "\n",
        "print(\"ZIP file created: processed_results.zip\")\n",
        "\n",
        "# Split ZIP into smaller parts (100 MB each)\n",
        "!split -b 100M /content/processed_results.zip /content/processed_results_part_\n",
        "\n",
        "# List generated parts\n",
        "split_parts = [f for f in os.listdir('/content') if f.startswith('processed_results_part_')]\n",
        "print(\"Generated parts:\")\n",
        "for part in split_parts:\n",
        "    print(part)\n",
        "\n",
        "# Download each part\n",
        "for part in split_parts:\n",
        "    files.download(part)\n",
        "\n",
        "print(\"All parts are ready for download.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding Slope"
      ],
      "metadata": {
        "id": "DqAQcFdU1QdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Constants for Stokes-Einstein relation\n",
        "K_B = 1.38e-23  # Boltzmann constant (J/K)\n",
        "T = 298  # Temperature in Kelvin\n",
        "ETA = 0.89e-3  # Viscosity in Pa·s (milliPascal·seconds converted to SI)\n",
        "MICROMETER_TO_METER = 1e-6  # Conversion from micrometer to meter\n",
        "\n",
        "# Step 1: Upload files\n",
        "excel_file = '/content/matched_particle_data_with_difference (5).csv'  # Replace with your Excel file path\n",
        "text_file = '/content/slope in log scale.txt'  # Replace with your text file path\n",
        "\n",
        "# Step 2: Try reading the Excel file\n",
        "try:\n",
        "    excel_data = pd.read_excel(excel_file, engine='openpyxl')  # Try using openpyxl for .xlsx files\n",
        "except Exception as e:\n",
        "    print(f\"Error reading Excel file as Excel: {e}\")\n",
        "    # If there's an error with Excel, try reading it as CSV\n",
        "    try:\n",
        "        print(\"Attempting to read as CSV...\")\n",
        "        excel_data = pd.read_csv(excel_file)  # Try reading it as CSV instead\n",
        "    except Exception as e2:\n",
        "        print(f\"Error reading file as CSV: {e2}\")\n",
        "        raise  # Raise the error if it fails both ways\n",
        "\n",
        "# Read the text file\n",
        "text_data = pd.read_csv(text_file, delim_whitespace=True, header=None, names=[\"Particle ID\", \"Slope (log scale)\"])\n",
        "\n",
        "# Step 3: Match and filter data by Particle ID\n",
        "matched_data = pd.merge(excel_data, text_data, on=\"Particle ID\", how=\"inner\")\n",
        "\n",
        "# Step 4: Calculate Diffusion Coefficients using Stokes-Einstein relation\n",
        "def calculate_diffusion_coefficient(radius_micrometers):\n",
        "    radius_meters = radius_micrometers * MICROMETER_TO_METER\n",
        "    return (K_B * T) / (6 * 3.14159 * ETA * radius_meters)\n",
        "\n",
        "# Adding the calculated diffusion coefficient\n",
        "matched_data[\"Calculated Diffusion (µm²/s)\"] = matched_data[\"Radius (µm)\"].apply(calculate_diffusion_coefficient)\n",
        "\n",
        "# Step 5: Compute the product of Radius and Diffusion Coefficient\n",
        "matched_data[\"Radius × Diffusion Coefficient\"] = matched_data[\"Radius (µm)\"] * matched_data[\"Diffusion Coefficient\"]\n",
        "\n",
        "# Step 6: Add the Relative Diffusion Coefficient\n",
        "matched_data[\"Relative Diffusion Coefficient (%)\"] = (\n",
        "    (matched_data[\"Diffusion Coefficient\"] - matched_data[\"Calculated Diffusion (µm²/s)\"]) /\n",
        "    matched_data[\"Calculated Diffusion (µm²/s)\"]\n",
        ") * 100\n",
        "\n",
        "# Step 7: Save results to a new Excel file\n",
        "output_file = '/content/output_matched_data_with_relative_diffusion.xlsx'\n",
        "matched_data.to_excel(output_file, index=False)\n",
        "\n",
        "# Step 8: Notify completion\n",
        "print(f\"Processing complete. Results saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "u1HO033n1QSu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}